{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T2 - Sentiment Analysis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/edgarbanhesse/ia369y-affective-computing/blob/master/T2_Sentiment_Analysis.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "RJ6O_vzulgeM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## T2 - Análise de Sentimentos em Textos\n",
        "\n",
        "### Objetivo\n",
        "\n",
        "O objetivo desta tarefa é expor os alunos aos desafios práticos envolvidos na análise de textos e na atribuição de valores de valência ou rótulos de emoções a sentenças.\n",
        "\n",
        "Não é objetivo desta tarefa avaliar a acurácia de detecção ou a eficiência do modelo implementado mas a análise crítica do projeto e o amadurecimento em relação ao problema.\n",
        "\n",
        "### Descrição da Tarefa\n",
        "\n",
        "Esta tarefa deverá ser realizada individualmente ou em dupla.\n",
        "\n",
        "Deve-se escolher um entre os dois problemas propostos abaixo.\n",
        "\n",
        "\n",
        "### Problema escolhido #2\n",
        "\n",
        " - Este problema utilizará a mesma base de dados utilizada no SemEval 2007 - 4th International Workshop on Semantic Evaluations, Task 14, Affective Tests.\n",
        " - A base de treinamento conta com 250 manchetes em inglês de jornais e websites (Google, CNN, etc.)\n",
        " - A cada manchete está associado um score de (0 a 100)  para os rótulos \"anger\", \"disgust\", \"fear\", \"joy\", \"sadness\", \"surprise\"\n",
        " - Também será fornecida uma base de testes e os rótulos \"golden\" fornecidos durante a conferência.\n",
        " - Todos os dados podem ser acessados pelo link, abaixo. Mas atenção, considerar apenas os dados referentes  a rótulos de emoções (identificados com sufixo *emotions*). A leitura dos arquivos README é essencial para o entendimento da base. http://web.eecs.umich.edu/~mihalcea/affectivetext/\n",
        " - A tarefa consiste em definir a abordagem ao problema, o modelo de classificação, as regras de análise e deverá realizar uma implementação prática do algoritmo definido.\n",
        "\n",
        "### Dupla\n",
        "\n",
        "- Edgar Lopes Banhesse RA 993396\n",
        "- Rodolfo De Nadai RA 208911"
      ]
    },
    {
      "metadata": {
        "id": "AsN2ZEyhmo9j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Iniciando a resolução do Problema"
      ]
    },
    {
      "metadata": {
        "id": "Sps57Gj26Fu5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Abordagem para resolução do problema\n",
        "\n",
        "Para resolver o problema a abordagem será composta pelas seguintes etapas:\n",
        "\n",
        "1) Pré-processamento do texto - consiste em preparar a base de treinamento que será utilizada para treinar o modelo de classificação.\n",
        "\n",
        "2) Escolha e treinamento do classificador - será utilizado o Naive Bayes.\n",
        "\n",
        "3) Avaliar o modelo - utilizar métricas, tais como: precisão (precision), revocação (recall) e medida F (f1-score) para avaliar a acurácia dos resultados obtidos com o modelo.\n",
        "\n",
        "4) Melhorar o modelo - treinar o modelo de classificação utilizando trigramas.\n",
        "\n",
        "5) Avaliar o modelo novamente - repetir o passo 3.\n",
        "\n",
        "6) Opotunidades - analisar os resultados obtidos em busca de oportunidades.\n",
        "- Vocês concordam com os rótulos que o algoritmo atribuiu?\n",
        "- Foi possível descobrir algo relevante sobre os dados a partir da análise de sentimentos?\n",
        "- Considerando que nenhum modelo é perfeito, quais são os pontos fracos do algoritmo implementado?\n",
        "- Quais seriam os pontos fortes?\n",
        "\n",
        "7) Conclusão - finalizar o experimento relatando as principais descobertas.\n",
        "\n",
        "8) Lições aprendidas - registrar as principais lições aprendidas durante a realização da tarefa."
      ]
    },
    {
      "metadata": {
        "id": "LAf3Sh69mvzG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 1. Download do Dataset a ser utilizado e descompactação do mesmo."
      ]
    },
    {
      "metadata": {
        "id": "YL9e49Mj3kU-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "outputId": "1a3c505e-f62e-483e-86a1-7bb7f391f941"
      },
      "cell_type": "code",
      "source": [
        "!wget http://web.eecs.umich.edu/~mihalcea/downloads/AffectiveText.Semeval.2007.tar.gz\n",
        "!tar -zxvf AffectiveText.Semeval.2007.tar.gz\n",
        "!ls\n",
        "!ls AffectiveText.trial\n",
        "\n",
        "!pip install torch torchvision"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Redirecting output to ‘wget-log’.\n",
            "AffectiveText.test/\n",
            "AffectiveText.test/affectivetext_test.xml\n",
            "AffectiveText.test/README\n",
            "AffectiveText.test/affectivetext_test.valence.gold\n",
            "AffectiveText.test/affectivetext_test.emotions.gold\n",
            "AffectiveText.trial/\n",
            "AffectiveText.trial/affectivetext_trial.valence.gold\n",
            "AffectiveText.trial/affectivetext_trial.emotions.gold\n",
            "AffectiveText.trial/affectivetext_trial.xml\n",
            "AffectiveText.trial/README\n",
            "AffectiveText.Semeval.2007.tar.gz  AffectiveText.trial\twget-log\n",
            "AffectiveText.test\t\t   sample_data\n",
            "affectivetext_trial.emotions.gold  affectivetext_trial.xml\n",
            "affectivetext_trial.valence.gold   README\n",
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/0e/e382bcf1a6ae8225f50b99cc26effa2d4cc6d66975ccf3fa9590efcbedce/torch-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (519.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 519.5MB 26kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x58f76000 @  0x7fea4366a1c4 0x46d6a4 0x5fcbcc 0x4c494d 0x54f3c4 0x553aaf 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54e4c8\n",
            "\u001b[?25hCollecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 21.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.5)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/24/f53ff6b61b3d728b90934bddb4f03f8ab584a7f49299bf3bde56e2952612/Pillow-5.2.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 3.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Installing collected packages: torch, pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.2.0 torch-0.4.1 torchvision-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NI3n_VROms0f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 2. Importação de bibliotecas."
      ]
    },
    {
      "metadata": {
        "id": "lybxYzk53UP2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "b7bd2f1a-3a31-45c1-9e25-55ea79aae46e"
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import namedtuple\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "np.warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download de alguns dataset disponibilizados pelo NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('sentence_polarity')\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('words')\n",
        "\n",
        "from nltk.corpus import movie_reviews\n",
        "# from nltk.corpus import sentence_polarity\n",
        "# from nltk.corpus import sentiwordnet\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.util import ngrams\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
        "\n",
        "Conjunto = namedtuple('Conjunto', ['uid', 'phrase',\n",
        "                                   'tokens',\n",
        "                                   'valence', 'anger',\n",
        "                                   'disgust', 'fear',\n",
        "                                   'joy', 'sadness', 'surprise'])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package sentence_polarity to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "505xGswbqOLR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3. Importando o Dataset do SemEval 2007 para uma estrutura.\n",
        "\n",
        " - **Phrases**: id, phrase\n",
        " - **Emotions**: id, anger, disgust, fear, joy, sadness, surprise \n",
        " - **Valence**: id, valence\n",
        " \n",
        "Valor de retorno da função load_semeval_dataset:\n",
        " \n",
        " - **Content**: uid, phrase, valence,  anger, disgust, fear, joy, sadness, surprise"
      ]
    },
    {
      "metadata": {
        "id": "e43HpT3ApZT3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def clean_phrase(phrase):\n",
        "    phrase = phrase.lower()\n",
        "    # Remove pontuação\n",
        "    phrase = re.sub(r'[\\\"\\'!@#$%&*\\(\\)-_=+{}\\[\\]:;>.<,|\\\\`´]', '', phrase)\n",
        "    # Remover stopwords em inglês e Lematização das palavras\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\n",
        "    # stwords = set(stopwords.words('english'))\n",
        "    stwords = set(ENGLISH_STOP_WORDS)\n",
        "    phrase = ' '.join([wordnet_lemmatizer.lemmatize(word) for word in phrase.split() if word not in stwords and len(word) > 2])\n",
        "    # Remove espaços em branco extras\n",
        "    phrase = re.sub(r'\\s{1,}', ' ', phrase)\n",
        "    return phrase\n",
        "\n",
        "\n",
        "def tokenize(phrase):\n",
        "    # Limpar e retorna trigramas da frase\n",
        "    return clean_phrase(phrase).split()\n",
        "\n",
        "\n",
        "def load_semeval_dataset(fname='test'):\n",
        "    content = []\n",
        "    phrases = []\n",
        "    emotions = []\n",
        "    valences = []\n",
        "    with open(f'AffectiveText.{fname}/affectivetext_{fname}.xml') as fxml:\n",
        "        parser = BeautifulSoup(fxml.read())\n",
        "        phrases = list(filter(None, parser.text.split('\\n')))\n",
        "    with open(f'AffectiveText.{fname}/affectivetext_{fname}.emotions.gold') as femotions:\n",
        "        emons = list(filter(None, femotions.read().split('\\n')))\n",
        "        for emotion in emons:\n",
        "            emon = [int(e) for e in emotion.split()]\n",
        "            emotions.append(emon)\n",
        "    with open(f'AffectiveText.{fname}/affectivetext_{fname}.valence.gold') as fvalences:\n",
        "        valens = list(filter(None, fvalences.read().split('\\n')))\n",
        "        for valence in valens:\n",
        "            valen = [int(v) for v in valence.split()]\n",
        "            valences.append(valen)\n",
        "    for i, phrase in enumerate(phrases):\n",
        "        content.append(Conjunto(uid=valences[i][0],\n",
        "                                phrase=phrase,\n",
        "                                tokens=tokenize(phrase),\n",
        "                                valence=valences[i][1:][0],\n",
        "                                anger=emotions[i][1],\n",
        "                                disgust=emotions[i][2],\n",
        "                                fear=emotions[i][3],\n",
        "                                joy=emotions[i][4],\n",
        "                                sadness=emotions[i][5], \n",
        "                                surprise=emotions[i][6]))\n",
        "    return content\n",
        "\n",
        "# Carregar os dados de test e trial\n",
        "train = load_semeval_dataset('test')\n",
        "test = load_semeval_dataset('trial')\n",
        "# train = load_semeval_dataset('trial')\n",
        "# test = load_semeval_dataset('test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JCW9urkQYrTM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4. TF-IDF\n",
        "\n",
        "Implementação do método estatístico TF-IDF e comparativo com a versão implementada pela biblioteca scikit-learn.\n",
        "\n",
        "Na biblioteca da scikit-learn é aplicada dentre outros procedimentos a normalização nos termos resultantes da tf-idf. A normalização utilizada é a distância euclidiana dos valores. É possível verificar uma explicação detalhada de alguns passos da implementação no [site](http://scikit-learn.org/stable/modules/feature_extraction.html) deles.\n",
        "\n",
        "Nossa implementação abaixo, segue o padrão apresentado na [wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
        "\n",
        "Onde:\n",
        "\n",
        "$tfidf(t, d, D) = tf(t,d) \\times idf(t, D)$\n",
        "\n",
        "$tf(t, d) = f_{t,d} / \\Sigma{f_{t',d}}$\n",
        "\n",
        "$idf(d, D) = \\log \\frac{|D|}{d}$\n",
        "\n",
        "Sendo $t$ o termo (palavra), $d$ o documento e $D$ o conjunto de documentos."
      ]
    },
    {
      "metadata": {
        "id": "3YRxfMLgsLol",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TfidfImpl:\n",
        "\n",
        "    def __init__(self, stopwords=None):\n",
        "        self.stopwords = stopwords\n",
        "    \n",
        "    def clean_phrase(self, phrase):\n",
        "        phrase = phrase.lower()\n",
        "        # Remove pontuação\n",
        "        phrase = re.sub(r'[\\\"\\'!@#$%&*\\(\\)-_=+{}\\[\\]:;>.<,|\\\\`´]', '', phrase)\n",
        "        if self.stopwords == 'english':\n",
        "            # Remover stopwords em inglês e Lematização das palavras\n",
        "            wordnet_lemmatizer = WordNetLemmatizer()\n",
        "            # stwords = set(stopwords.words('english'))\n",
        "            stwords = set(ENGLISH_STOP_WORDS)\n",
        "            phrase = ' '.join([wordnet_lemmatizer.lemmatize(word) for word in phrase.split() if word not in stwords and len(word) > 2])\n",
        "        # Remove espaços em branco extras\n",
        "        phrase = re.sub(r'\\s{1,}', ' ', phrase)\n",
        "        return phrase\n",
        "\n",
        "\n",
        "    def tokenize(self, phrase):\n",
        "        # Limpar e retorna trigramas da frase\n",
        "        return self.clean_phrase(phrase).split()\n",
        "    \n",
        "    def bag_of_words(self, phrases):\n",
        "        bow = []\n",
        "        for phrase in phrases:\n",
        "            bow += phrase\n",
        "        return sorted(set(bow))\n",
        "\n",
        "\n",
        "    def compute_tf(self, words):\n",
        "        tf = {}\n",
        "        lbow = len(words)\n",
        "        for word in words:\n",
        "            tf[word] = tf.get(word, 0) + 1\n",
        "        for word, count in tf.items():\n",
        "            tf[word] = count / lbow\n",
        "        return tf\n",
        "\n",
        "\n",
        "    def compute_idf(self, phrases, N, bow):\n",
        "        idfs = {}\n",
        "        for df in bow:\n",
        "            idfs[df] = idfs.get(df, 0)\n",
        "            for words in phrases:\n",
        "                if df in words:\n",
        "                    idfs[df] = idfs.get(df, 0) + 1\n",
        "        for df in idfs.keys():\n",
        "            idfs[df] = np.log10(N / idfs[df])\n",
        "        return idfs\n",
        "\n",
        "\n",
        "    def compute(self, phrases):\n",
        "        # Checagem... e conversão\n",
        "        assert len(phrases) > 0\n",
        "        if type(phrases[0]) is str:\n",
        "            phrases = [self.tokenize(phrase) for phrase in phrases]\n",
        "        \n",
        "        tf_idf = {}\n",
        "        N = len(phrases)\n",
        "        bow = self.bag_of_words(phrases)\n",
        "        idf = self.compute_idf(phrases, N, bow)\n",
        "        for words in phrases:\n",
        "            tf = self.compute_tf(words)\n",
        "            for word, val in tf.items():\n",
        "                tf_idf[word] = val * idf[word]\n",
        "        return tf_idf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BbQTPq4mC2px",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Resultados das funções criadas acima:"
      ]
    },
    {
      "metadata": {
        "id": "KWH4t9MqC0lA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 713
        },
        "outputId": "c3541a04-3a5d-488c-cf3b-b56c620e4053"
      },
      "cell_type": "code",
      "source": [
        "# Convertendo o conjunto montado acima para apenas as frases\n",
        "# phrases = []\n",
        "# conjuntos = test + trial\n",
        "# for conj in conjuntos:\n",
        "#    words = conj.tokens\n",
        "#    phrases.append(' '.join(words))\n",
        "\n",
        "# Testes  \n",
        "phrases = ['The cat is in the hole', 'The rat is in the hole', 'The dog are looking at you througth the hole']\n",
        "\n",
        "# Mostrando apenas as primeiras palavras com menor valor\n",
        "head = 5\n",
        "\n",
        "print('Nossa implementação: ')\n",
        "Tfidf = TfidfImpl(stopwords='english')\n",
        "tf_idf = Tfidf.compute(phrases)\n",
        "df = pd.DataFrame({'term': list(tf_idf.keys()), 'weight': list(tf_idf.values())})\n",
        "df = df.sort_values(by='weight', ascending=True)\n",
        "display(df.head(head))\n",
        "\n",
        "print()\n",
        "print('-' * 20)\n",
        "print('Implementação do Scikit-Learn: TfidfVectorizer')\n",
        "vectorizer = TfidfVectorizer(stop_words='english', norm=None, smooth_idf=False)\n",
        "transformed_weights = vectorizer.fit_transform(phrases)\n",
        "weights = np.asarray(transformed_weights.mean(axis=0)).ravel().tolist()\n",
        "weights_df = pd.DataFrame({'term': vectorizer.get_feature_names(), 'weight': weights})\n",
        "weights_df = weights_df.sort_values(by='weight', ascending=True)\n",
        "display(weights_df.head(head))\n",
        "\n",
        "print()\n",
        "print('-' * 20)\n",
        "print('Implementação do Scikit-Learn: CountVectorizer + TfidfTransformer')\n",
        "cvec = CountVectorizer(stop_words='english')\n",
        "cvec.fit(phrases)\n",
        "cvec_counts = cvec.transform(phrases)\n",
        "transformer = TfidfTransformer(norm=None, smooth_idf=False)\n",
        "transformed_weights = transformer.fit_transform(cvec_counts)\n",
        "weights = np.asarray(transformed_weights.mean(axis=0)).ravel().tolist()\n",
        "weights_df = pd.DataFrame({'term': cvec.get_feature_names(), 'weight': weights})\n",
        "weights_df = weights_df.sort_values(by='weight', ascending=True)\n",
        "display(weights_df.head(head))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nossa implementação: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>term</th>\n",
              "      <th>weight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hole</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>dog</td>\n",
              "      <td>0.119280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>looking</td>\n",
              "      <td>0.119280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>througth</td>\n",
              "      <td>0.119280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cat</td>\n",
              "      <td>0.238561</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       term    weight\n",
              "1      hole  0.000000\n",
              "3       dog  0.119280\n",
              "4   looking  0.119280\n",
              "5  througth  0.119280\n",
              "0       cat  0.238561"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Implementação do Scikit-Learn: TfidfVectorizer\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>term</th>\n",
              "      <th>weight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cat</td>\n",
              "      <td>0.699537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>dog</td>\n",
              "      <td>0.699537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>looking</td>\n",
              "      <td>0.699537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>rat</td>\n",
              "      <td>0.699537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>througth</td>\n",
              "      <td>0.699537</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       term    weight\n",
              "0       cat  0.699537\n",
              "1       dog  0.699537\n",
              "3   looking  0.699537\n",
              "4       rat  0.699537\n",
              "5  througth  0.699537"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------\n",
            "Implementação do Scikit-Learn: CountVectorizer + TfidfTransformer\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>term</th>\n",
              "      <th>weight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cat</td>\n",
              "      <td>0.699537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>dog</td>\n",
              "      <td>0.699537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>looking</td>\n",
              "      <td>0.699537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>rat</td>\n",
              "      <td>0.699537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>througth</td>\n",
              "      <td>0.699537</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       term    weight\n",
              "0       cat  0.699537\n",
              "1       dog  0.699537\n",
              "3   looking  0.699537\n",
              "4       rat  0.699537\n",
              "5  througth  0.699537"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Pq8GqfNawa0Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a9bcf677-8389-4437-ab11-2aa985b2ab88"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "def retorna_frases(conjunto):\n",
        "    frases = [conj.phrase for conj in conjunto]\n",
        "    valencia = [conj.valence for conj in conjunto]\n",
        "    return frases, valencia\n",
        "\n",
        "# Convertendo o conjunto montado acima para apenas as frases\n",
        "p_train_frases, p_train_valencia = retorna_frases(train)\n",
        "p_test_frases, p_test_valencia = retorna_frases(test)\n",
        "\n",
        "predict_frases = [\n",
        "    'Bad news comes first, good news after.',\n",
        "    'I enjoy too much this movie, i love it.',\n",
        "    'Hate you, and your brother!',\n",
        "    'Great day to walk in the park and play with my dog!',\n",
        "    'Don\\'t you worry my friend.',\n",
        "    'I have bad news for you sir.',\n",
        "]\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 5))\n",
        "train_transformed_weights = vectorizer.fit_transform(p_train_frases)\n",
        "test_transformed_weights = vectorizer.transform(p_test_frases)\n",
        "validate_transformed_weights = vectorizer.transform(predict_frases)\n",
        "\n",
        "modelo = MultinomialNB()\n",
        "modelo.fit(train_transformed_weights, p_train_valencia)\n",
        "print(modelo.score(train_transformed_weights, p_train_valencia))\n",
        "print(modelo.predict(validate_transformed_weights))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.653\n",
            "[ 38  47 -64  38  38  38]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qtMIFH6XBpY2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "12e14cbd-cdaa-4380-d051-da9d6d11d587"
      },
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "datasets = np.array(p_train_frases + p_test_frases)\n",
        "values = np.array(p_train_valencia + p_test_valencia)\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 5))\n",
        "crossval_transformed_weights = vectorizer.fit_transform(datasets)\n",
        "\n",
        "predicted = cross_val_predict(modelo, crossval_transformed_weights, values, cv=6)\n",
        "metrics.accuracy_score(values, predicted)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0136"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    }
  ]
}